{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45f20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362b1aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3637955906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m from utils.data_loader import (\n\u001b[1;32m     12\u001b[0m     \u001b[0mHDF5DatasetExplorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training script for pre-trained brain tumor segmentation model.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.data_loader import (\n",
    "    HDF5DatasetExplorer,\n",
    "    create_data_splits,\n",
    "    load_data_splits,\n",
    "    create_dataloaders\n",
    ")\n",
    "from preprocessing.augmentation import AugmentationTransform, PreprocessingTransform\n",
    "from models.pretrained_model import get_pretrained_model\n",
    "from training.losses import DiceLoss, CombinedLoss\n",
    "from training.trainer import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    config.create_directories()\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(config.RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(config.RANDOM_SEED)\n",
    "    import numpy as np\n",
    "    np.random.seed(config.RANDOM_SEED)\n",
    "    import random\n",
    "    random.seed(config.RANDOM_SEED)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Explore dataset\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Exploring Dataset\")\n",
    "    print(\"=\"*60)\n",
    "    explorer = HDF5DatasetExplorer(config.HDF5_DATASET_PATH)\n",
    "    explorer.print_summary()\n",
    "    \n",
    "    # Create or load data splits\n",
    "    splits_dir = config.SPLITS_DIR\n",
    "    if not (splits_dir / \"train_ids.json\").exists():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Creating Data Splits\")\n",
    "        print(\"=\"*60)\n",
    "        splits = create_data_splits(\n",
    "            config.HDF5_DATASET_PATH,\n",
    "            splits_dir,\n",
    "            train_ratio=config.TRAIN_SPLIT,\n",
    "            val_ratio=config.VAL_SPLIT,\n",
    "            test_ratio=config.TEST_SPLIT,\n",
    "            random_seed=config.RANDOM_SEED,\n",
    "            stratify=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Loading Existing Data Splits\")\n",
    "        print(\"=\"*60)\n",
    "        splits = load_data_splits(splits_dir)\n",
    "        for split_name, ids in splits.items():\n",
    "            print(f\"{split_name.upper()}: {len(ids)} patients\")\n",
    "    \n",
    "    # Create transforms\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Setting up Preprocessing and Augmentation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    preprocessing = PreprocessingTransform(\n",
    "        target_size=config.IMAGE_SIZE,\n",
    "        denoising_method=config.DENOISING_METHOD,\n",
    "        normalization_method=config.NORMALIZATION_METHOD\n",
    "    )\n",
    "    \n",
    "    augmentation = AugmentationTransform(\n",
    "        rotation_range=config.ROTATION_RANGE,\n",
    "        translation_range=config.TRANSLATION_RANGE,\n",
    "        scale_range=config.SCALE_RANGE,\n",
    "        brightness_range=config.BRIGHTNESS_RANGE,\n",
    "        contrast_range=config.CONTRAST_RANGE,\n",
    "        flip_probability=config.FLIP_PROBABILITY\n",
    "    ) if config.AUGMENTATION_ENABLED else None\n",
    "    \n",
    "    # Compose transforms\n",
    "    def train_transform(sample):\n",
    "        sample = preprocessing(sample)\n",
    "        if augmentation:\n",
    "            sample = augmentation(sample)\n",
    "        return sample\n",
    "    \n",
    "    # Create data loaders\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating Data Loaders\")\n",
    "    print(\"=\"*60)\n",
    "    dataloaders = create_dataloaders(\n",
    "        config.HDF5_DATASET_PATH,\n",
    "        splits,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=preprocessing,\n",
    "        return_label=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(dataloaders['train'])}\")\n",
    "    print(f\"Val batches: {len(dataloaders['val'])}\")\n",
    "    print(f\"Test batches: {len(dataloaders['test'])}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating Pre-trained Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Determine model type and encoder\n",
    "    if config.PRETRAINED_MODEL_TYPE.lower() == \"resnet\":\n",
    "        model_type = \"resnet\"\n",
    "        encoder_name = config.PRETRAINED_ENCODER\n",
    "    elif config.PRETRAINED_MODEL_TYPE.lower() == \"vgg\":\n",
    "        model_type = \"vgg\"\n",
    "        encoder_name = \"vgg16\"  # Default VGG\n",
    "    else:\n",
    "        model_type = \"resnet\"\n",
    "        encoder_name = \"resnet50\"\n",
    "    \n",
    "    model = get_pretrained_model(\n",
    "        model_type=model_type,\n",
    "        encoder_name=encoder_name,\n",
    "        pretrained=True,\n",
    "        num_classes=1,\n",
    "        dropout=config.DROPOUT_RATE\n",
    "    ).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = DiceLoss(smooth=1e-6)\n",
    "    # Alternative: CombinedLoss(dice_weight=0.5, bce_weight=0.5)\n",
    "    \n",
    "    # Optimizer with differential learning rates\n",
    "    # Lower LR for encoder (pre-trained), higher for decoder (new)\n",
    "    encoder_params = []\n",
    "    decoder_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder' in name:\n",
    "            encoder_params.append(param)\n",
    "        else:\n",
    "            decoder_params.append(param)\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': encoder_params, 'lr': config.LEARNING_RATE * 0.1},  # 10x lower LR for encoder\n",
    "        {'params': decoder_params, 'lr': config.LEARNING_RATE}\n",
    "    ], weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=dataloaders['train'],\n",
    "        val_loader=dataloaders['val'],\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        model_name=f\"pretrained_{model_type}_{encoder_name}\"\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting Training\")\n",
    "    print(\"=\"*60)\n",
    "    trainer.train(num_epochs=config.NUM_EPOCHS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best validation DSC: {trainer.best_val_dice:.4f}\")\n",
    "    print(f\"Model saved to: {config.SAVED_MODELS_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a372104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Run Training\n",
    "# Execute this cell to start the training process\n",
    "\n",
    "print(\"ðŸš€ Starting training process...\\n\")\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a36a86",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**1. ModuleNotFoundError: No module named 'utils'**\n",
    "- Solution: Make sure you ran Cell 3 and that your project files are properly uploaded\n",
    "- Check that folders like `utils/`, `models/`, `preprocessing/` exist in your current directory\n",
    "- Try setting `PROJECT_ROOT` manually in Cell 3\n",
    "\n",
    "**2. ModuleNotFoundError: No module named 'torch'**\n",
    "- Solution: Run Cell 2 to install all required packages\n",
    "- Restart the runtime if needed: Runtime â†’ Restart Runtime\n",
    "\n",
    "**3. FileNotFoundError: brain_tumor_dataset.h5 not found**\n",
    "- Solution: Make sure you uploaded the `dataset/brain_tumor_dataset.h5` file\n",
    "- Check the path in `utils/config.py` matches your file location\n",
    "\n",
    "**4. CUDA out of memory**\n",
    "- Solution: Reduce batch size in `utils/config.py` (try 8 or 4 instead of 16)\n",
    "- Or use Runtime â†’ Change runtime type â†’ Select CPU (slower but works)\n",
    "\n",
    "**5. Training is very slow**\n",
    "- Solution: Use GPU runtime in Colab: Runtime â†’ Change runtime type â†’ GPU\n",
    "- Check GPU is being used by looking for \"Using device: cuda\" in the output\n",
    "\n",
    "**6. Need to modify training settings**\n",
    "- Edit `utils/config.py` before running the training\n",
    "- Key settings: `BATCH_SIZE`, `NUM_EPOCHS`, `LEARNING_RATE`, `IMAGE_SIZE`\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps After Training:\n",
    "\n",
    "1. Download your trained model from `/content/models/saved/`\n",
    "2. Use `evaluate_models.py` to evaluate performance\n",
    "3. Compare with custom model results\n",
    "4. Visualize predictions using evaluation scripts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
